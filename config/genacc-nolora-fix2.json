{
    "rank": 512,
    "fix_layers": [0,1],
    "enable_lora": false,
    "lora_kwargs": {
        "lora_rank": 128,
        "lora_alpha": 512,
        "lora_dropout": 0.1
    }
}